"""
ProteoPredict - Data Download Module
Downloads large protein dataset with controlled GO terms
"""

import requests
import pandas as pd
from pathlib import Path
import numpy as np
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import json

class ProteoDataDownloader:
    """Download and prepare protein data"""
    
    def __init__(self, output_dir="data/raw"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.base_url = "://rest.uniprot.org/uniprotkb/stream"
        https
    def download_uniprot_batch(self, query, max_proteins=10000):
        """
        Download large batch of proteins from UniProt
        """
        print(f"\n{'='*70}")
        print(f"DOWNLOADING PROTEIN DATA FROM UNIPROT")
        print(f"{'='*70}")
        print(f"Query: {query}")
        print(f"Target: {max_proteins} proteins")
        print(f"{'='*70}\n")
        
        params = {
            "query": query,
            "format": "tsv",
            "fields": "accession,id,protein_name,gene_names,organism_name,length,sequence,go,go_p,go_c,go_f,cc_function",
            "size": 500
        }
        
        try:
            print("Downloading in batches...")
            response = requests.get(self.base_url, params=params, stream=True, timeout=120)
            
            if response.status_code == 200:
                raw_file = self.output_dir / "uniprot_raw.tsv"
                
                with open(raw_file, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                print(f"âœ“ Raw data downloaded to {raw_file}")
                
                df = pd.read_csv(raw_file, sep='\t', nrows=max_proteins)
                print(f"\nâœ“ Successfully downloaded {len(df)} proteins")
                
                df_clean = self.clean_dataframe(df)
                
                output_file = self.output_dir / "proteins_complete.csv"
                df_clean.to_csv(output_file, index=False)
                
                print(f"âœ“ Saved cleaned data to {output_file}")
                
                return df_clean
                
            else:
                print(f"âœ— Error: HTTP {response.status_code}")
                print("Falling back to synthetic data generation...")
                return None
                
        except Exception as e:
            print(f"âœ— Download error: {e}")
            print("Generating synthetic dataset instead...")
            return None
    
    def clean_dataframe(self, df):
        """Clean and standardize dataframe"""
        print("\nCleaning data...")
        
        column_mapping = {
            'Entry': 'accession',
            'Entry Name': 'entry_name',
            'Protein names': 'protein_name',
            'Gene Names': 'gene_names',
            'Organism': 'organism',
            'Length': 'length',
            'Sequence': 'sequence',
            'Gene Ontology (GO)': 'go_all',
            'Gene Ontology (biological process)': 'go_bp',
            'Gene Ontology (molecular function)': 'go_mf',
            'Gene Ontology (cellular component)': 'go_cc',
            'Function [CC]': 'function_description'
        }
        
        df = df.rename(columns=column_mapping)
        df = df[df['sequence'].notna()].copy()
        df = df[df['length'] >= 50].copy()
        
        print(f"âœ“ Cleaned data: {len(df)} proteins remaining")
        
        return df
    
    def generate_large_synthetic_dataset(self, num_proteins=100000):
        """
        Generate large synthetic dataset for development
        WITH LIMITED GO TERMS FOR BETTER LEARNING
        """
        print(f"\n{'='*70}")
        print(f"GENERATING SYNTHETIC DATASET")
        print(f"{'='*70}")
        print(f"Creating {num_proteins} synthetic proteins...")
        print("(Limited to 28 common GO terms for better model performance)")
        print(f"{'='*70}\n")
        
        amino_acids = list("ACDEFGHIKLMNPQRSTVWY")
        
        # âœ… LIMITED GO TERMS - Only 28 common, learnable terms!
        go_terms = {
            'MF': [
                'GO:0003677',  # DNA binding
                'GO:0005524',  # ATP binding
                'GO:0003700',  # transcription factor activity
                'GO:0008270',  # zinc ion binding
                'GO:0005515',  # protein binding
                'GO:0004672',  # protein kinase activity
                'GO:0003723',  # RNA binding
                'GO:0016740',  # transferase activity
                'GO:0000166',  # nucleotide binding
                'GO:0046872',  # metal ion binding
            ],
            'BP': [
                'GO:0006355',  # regulation of transcription
                'GO:0006468',  # protein phosphorylation
                'GO:0007165',  # signal transduction
                'GO:0006412',  # translation
                'GO:0006810',  # transport
                'GO:0008283',  # cell proliferation
                'GO:0006351',  # transcription, DNA-templated
                'GO:0055114',  # oxidation-reduction process
                'GO:0006508',  # proteolysis
                'GO:0007049',  # cell cycle
            ],
            'CC': [
                'GO:0005634',  # nucleus
                'GO:0005737',  # cytoplasm
                'GO:0005886',  # plasma membrane
                'GO:0005739',  # mitochondrion
                'GO:0005783',  # endoplasmic reticulum
                'GO:0005576',  # extracellular region
                'GO:0016020',  # membrane
                'GO:0005654',  # nucleoplasm
            ]
        }
        
        print(f"GO Term Distribution:")
        print(f"  Molecular Function: {len(go_terms['MF'])} terms")
        print(f"  Biological Process: {len(go_terms['BP'])} terms")
        print(f"  Cellular Component: {len(go_terms['CC'])} terms")
        print(f"  Total: {len(go_terms['MF']) + len(go_terms['BP']) + len(go_terms['CC'])} GO terms\n")
        
        data = []
        
        print("Generating proteins...")
        for i in tqdm(range(num_proteins)):
            # Realistic length distribution (log-normal)
            length = int(np.random.lognormal(mean=6.0, sigma=0.6))
            length = max(50, min(length, 2000))
            
            # Generate sequence
            sequence = ''.join(np.random.choice(amino_acids, length))
            
            # Assign 2-8 GO terms per protein
            num_go = np.random.randint(2, 9)
            
            go_mf = list(np.random.choice(go_terms['MF'], size=min(3, num_go), replace=False))
            go_bp = list(np.random.choice(go_terms['BP'], size=min(3, num_go), replace=False))
            go_cc = list(np.random.choice(go_terms['CC'], size=min(2, num_go), replace=False))
            
            data.append({
                'accession': f'SYNTH{i:06d}',
                'entry_name': f'PROT{i}_HUMAN',
                'protein_name': f'Synthetic protein {i}',
                'gene_names': f'GENE{i}',
                'organism': 'Homo sapiens (Human)',
                'length': length,
                'sequence': sequence,
                'go_mf': ';'.join(go_mf),
                'go_bp': ';'.join(go_bp),
                'go_cc': ';'.join(go_cc),
                'go_all': ';'.join(go_mf + go_bp + go_cc),
                'function_description': f'Synthetic function description for protein {i}'
            })
        
        df = pd.DataFrame(data)
        
        # Save complete dataset
        output_file = self.output_dir / "proteins_complete.csv"
        df.to_csv(output_file, index=False)
        
        print(f"\nâœ“ Generated {len(df)} proteins")
        print(f"âœ“ Saved to {output_file}")
        print(f"âœ“ File size: {output_file.stat().st_size / 1024 / 1024:.1f} MB")
        
        # Show statistics
        print(f"\n{'='*70}")
        print("DATASET STATISTICS")
        print(f"{'='*70}")
        print(f"Total proteins: {len(df):,}")
        print(f"Avg sequence length: {df['length'].mean():.0f} amino acids")
        print(f"Length range: {df['length'].min()} - {df['length'].max()}")
        print(f"Unique GO terms: {len(set(';'.join(df['go_all']).split(';')))}")
        
        return df
    
    def create_train_val_test_split(self, df, train_size=0.7, val_size=0.15):
        """Split data into train/validation/test sets"""
        print(f"\n{'='*70}")
        print("CREATING TRAIN/VAL/TEST SPLITS")
        print(f"{'='*70}")
        
        # First split: train and temp (val+test)
        train_df, temp_df = train_test_split(
            df, 
            test_size=(1 - train_size), 
            random_state=42
        )
        
        # Second split: val and test
        val_ratio = val_size / (1 - train_size)
        val_df, test_df = train_test_split(
            temp_df,
            test_size=(1 - val_ratio),
            random_state=42
        )
        
        # Save splits
        train_df.to_csv(self.output_dir / "train.csv", index=False)
        val_df.to_csv(self.output_dir / "val.csv", index=False)
        test_df.to_csv(self.output_dir / "test.csv", index=False)
        
        print(f"âœ“ Train set: {len(train_df):,} proteins ({len(train_df)/len(df)*100:.1f}%)")
        print(f"âœ“ Validation set: {len(val_df):,} proteins ({len(val_df)/len(df)*100:.1f}%)")
        print(f"âœ“ Test set: {len(test_df):,} proteins ({len(test_df)/len(df)*100:.1f}%)")
        print(f"\nSaved to:")
        print(f"  - {self.output_dir / 'train.csv'}")
        print(f"  - {self.output_dir / 'val.csv'}")
        print(f"  - {self.output_dir / 'test.csv'}")
        
        return train_df, val_df, test_df


def main():
    """Main execution"""
    print("\n" + "ðŸ§¬ "*35)
    print("PROTEOPREDICT - DATA DOWNLOAD MODULE")
    print("ðŸ§¬ "*35 + "\n")
    
    downloader = ProteoDataDownloader()
    
    # FORCE SYNTHETIC DATA GENERATION (better for training)
    print("ðŸ“Š SYNTHETIC DATA GENERATION")
    print("=" * 70)
    print("Using synthetic data with controlled GO terms for better learning")
    print("Real UniProt data has 20K+ GO terms (too many for effective training)")
    print("=" * 70 + "\n")
    
    choice = input("Generate how many proteins? (Recommended: 100000): ").strip()
    num_proteins = int(choice) if choice.isdigit() and int(choice) > 0 else 100000
    
    print(f"\nâœ“ Will generate {num_proteins:,} proteins with 28 GO terms\n")
    
    df = downloader.generate_large_synthetic_dataset(num_proteins)
    
    # Create splits
    train_df, val_df, test_df = downloader.create_train_val_test_split(df)
    
    # Save GO vocabulary
    print(f"\n{'='*70}")
    print("EXTRACTING GO VOCABULARY")
    print(f"{'='*70}")
    
    all_go_terms = set()
    for col in ['go_mf', 'go_bp', 'go_cc']:
        for terms in df[col].dropna():
            if terms:
                all_go_terms.update(str(terms).split(';'))
    
    go_vocab = {
        'all_terms': sorted(list(all_go_terms)),
        'num_total': len(all_go_terms)
    }
    
    vocab_file = downloader.output_dir / "go_vocabulary.json"
    with open(vocab_file, 'w') as f:
        json.dump(go_vocab, f, indent=2)
    
    print(f"âœ“ Total GO terms: {go_vocab['num_total']}")
    print(f"âœ“ Saved vocabulary to: {vocab_file}")
    
    # Summary
    print(f"\n{'='*70}")
    print("âœ… DATA DOWNLOAD COMPLETE!")
    print(f"{'='*70}")
    print(f"\nDataset ready:")
    print(f"  Total proteins: {len(df):,}")
    print(f"  Training: {len(train_df):,}")
    print(f"  Validation: {len(val_df):,}")
    print(f"  Testing: {len(test_df):,}")
    print(f"  GO terms: {go_vocab['num_total']}")
    print(f"\nNext step: Run preprocessing")
    print(f"  python -m src.proteopredict.data.preprocess")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()